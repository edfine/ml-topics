{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1 Recommenders\n",
    "\n",
    "## Overview\n",
    "\n",
    "Introduction and level setting\n",
    "\n",
    "Review the types of Recommender Systems \n",
    "\n",
    "1. Rule\n",
    "\n",
    "2. Content-Based\n",
    "\n",
    "3. Collaborative Filtering\n",
    "\n",
    "4. Hybrid\n",
    "\n",
    "Implement a python version of the Collaborative model with small data set   \n",
    "\n",
    "\n",
    "Describe the Collaborative model (ALS) and how it is implemented\n",
    "\n",
    "\n",
    "Lab: Building Recommenders \n",
    "\n",
    "\n",
    "Recommenders at larger scale\n",
    "\n",
    "\n",
    "Lab: Using Spark to build recommenders\n",
    "\n",
    "\n",
    "Evaluation of Recommender Systems\n",
    "1. RMSE\n",
    "2. Precision & Recall\n",
    "3. DCG \n",
    "\n",
    "\n",
    "Practical issues when using Recommender Systems\n",
    "1. Latency\n",
    "2. Memory\n",
    "3. Fallbacks\n",
    "4. In-session updating\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-personalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-personalized recommendations are usually where most sites start because it’s easy and doesn’t require that you know anything specific about the users. Non-personalized recommendations are good because you can **always show those**, despite how little you know about the users. People might say that non-personalized recs should only be shown until the system knows enough about the user to show more personalized recs, but always remember that humans are flock animals by nature, so most will be suckers for knowing what content items are the most popular—if for no other reason than to ensure what not to like.\n",
    "\n",
    "![Cupon non personalized](img/cupon_alt.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cupon.com uses non-personalized recommendations to recommend more offers. In the top, it lists the popular categories and brands. While the central part of the screen contains lists of vouchers to save money on, it’s hard to say how that’s calculated. Cupon.com is one of many choices, and it’s a way for sellers to get in contact with people who are happy to get things cheaper—in quantity, thus spending more money.\n",
    "\n",
    "A recommendation, personalized or not, is based on data and calculated from data. To keep the definition from being too murky, we’ll restrict it to be computer-calculated based on usage data. That means popular categories at cupon.com are recommendations (by calculating which category is viewed more). Before you start calculating, let’s look at examples of what a site can do if it doesn’t have any data at all.\n",
    "\n",
    "![Dzone](img/dzone.jpg)\n",
    "\n",
    "Above we see how editors or tastemakers may produce non-personalised recs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Frequently bought items similar to the one you’re viewing\n",
    "\n",
    "Can you create an FBT recommender by finding all the products that are bought together with the current product and then take the top X of that? You could, but as you’ll see later, it doesn’t work well.\n",
    "\n",
    "One of the challenges of showing an FBT recommendation is that most products are bought together with other popular items. A classic example is that most people leaving a supermarket in Denmark will have a liter of milk in the bag, so almost no matter what else is in the basket, you could say that those items were frequently bought together with a liter of milk.\n",
    "\n",
    "As I was writing this, my wife came home from the supermarket with not only one, but two liters of milk (figure 5.11 shows the shopping receipt). Most products in a supermarket that are bought often will also be purchased with milk. This yields frequency sets containing milk and most other products. When two or more items are often seen together it’s called a frequency set.\n",
    "\n",
    "#### Association rules\n",
    "Instead of looking at most popular objects, there’s the idea of association rules, something a bit closer to kindness than marketing talk. Association rules in the commerce scenario can be thought of as well-meaning advice. Most people hate to come home with a new hard disk only to realize that they don’t have any cable to connect it. If you buy things on Amazon, then you’re in luck because they remind you that most people buy a cable with the hard drive, as you can see:\n",
    "\n",
    "![Amazon](img/amazon.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition: Confidence\n",
    "\n",
    "![Conf](img/confidence.jpg)\n",
    "\n",
    "where T(X) is the set of transactions that contain X\n",
    "\n",
    "\n",
    "Let’s calculate what the confidence rating is that milk will be in the basket when bread is also bought. This can be written like this:\n",
    "\n",
    "![Support](img/support.jpg)\n",
    "\n",
    "Next you need to find all the transactions containing first both bread and milk and then only bread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing association rules\n",
    "The procedure described in the previous section goes something like the following:\n",
    "\n",
    "\n",
    "1.  Settle on a minimum support and minimum confidence level.\n",
    "\n",
    "2.  Get all transactions.\n",
    "\n",
    "3.  Create a list of itemsets, one for each element, and calculate their support (number of times it’s present divided by the number of transactions) and set confidence to one.\n",
    "\n",
    "4.  Build a list of itemsets containing more than one item and calculate support and confidence by inferring that each transaction finds all combinations of items and adds one to the itemset’s support.\n",
    "\n",
    "5.  Iterate through the itemsets and remove the ones that don’t fulfill the confidence requirement.\n",
    "\n",
    "Let’s translate this into Python code, but we’ll wait a bit before setting the minimum support and confidence level, calculating everything to start with.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-2-8d63169a9abc>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-8d63169a9abc>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    sql = \"\"\"\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def retrieve_buy_events():\n",
    "\n",
    "    sql = \"\"\"\n",
    "    SELECT *\n",
    "    FROM  Collector_log\n",
    "    WHERE event = 'buy'\n",
    "    ORDER BY session_id, content_id\n",
    "    \"\"\"                                            \n",
    "\n",
    "    cursor = data_helper.get_query_cursor(sql)\n",
    "    data = data_helper.dictfetchall(cursor)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_transactions(data):\n",
    "    transactions = dict()\n",
    "\n",
    "    for trans_item in data:                                  \n",
    "        id = trans_item[\"session_id\"]                        \n",
    "        if id not in transactions:                           \n",
    "            transactions[id] = []\n",
    "        transactions[id].append(trans_item[\"content_id\"])    \n",
    "\n",
    "    return transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_support_confidence(transactions, min_sup=0.01):\n",
    "\n",
    "    N = len(transactions)                                          \n",
    "    one_itemsets = calculate_itemsets_one(transactions, min_sup)   \n",
    "    two_itemsets = calculate_itemsets_two(transactions,\n",
    "                            one_itemsets, min_sup)                 \n",
    "    rules = calculate_association_rules(one_itemsets,\n",
    "                                        two_itemsets, N)           \n",
    "\n",
    "    return sorted(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_itemsets_one(transactions, min_sup=0.01):\n",
    "\n",
    "    N = len(transactions)                               \n",
    "\n",
    "    temp = defaultdict(int)                             \n",
    "    one_itemsets = dict()\n",
    "\n",
    "    for key, items in transactions.items():             \n",
    "        for item in items:                              \n",
    "            inx = frozenset({item})                     \n",
    "            temp[inx] += 1                              \n",
    "\n",
    "    # remove all items that is not supported.\n",
    "    for key, itemset in temp.items():                   \n",
    "        if itemset > min_sup * N:                       \n",
    "            one_itemsets[key] = itemset\n",
    "\n",
    "    return one_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_itemsets_two(transactions, one_itemsets, min_sup=0.01):\n",
    "    two_itemsets = defaultdict(int)\n",
    "\n",
    "    for key, items in transactions.items():                        \n",
    "        items = list(set(items))                                   \n",
    "\n",
    "        if (len(items) > 2):                                       \n",
    "            for perm in combinations(items, 2):                    \n",
    "                if has_support(perm, one_itemsets):                \n",
    "                    two_itemsets[frozenset(perm)] += 1             \n",
    "\n",
    "        elif len(items) == 2:                                      \n",
    "            if has_support(items, one_itemsets):                   \n",
    "                two_itemsets[frozenset(items)] += 1                \n",
    "    return two_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_association_rules(one_itemsets, two_itemsets, N):\n",
    "    timestamp = datetime.now()\n",
    "\n",
    "    rules = []\n",
    "    for source, source_freq in one_itemsets.items():                       \n",
    "        for key, group_freq in two_itemsets.items():                       \n",
    "            if source.issubset(key):                                       \n",
    "                target = key.difference(source)                            \n",
    "                support = group_freq / N                                   \n",
    "                confidence = group_freq / source_freq                      \n",
    "                rules.append((timestamp, next(iter(source)), next(iter(target)), confidence, support))                                                             \n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_association_rules_for(request, content_id, take=6):\n",
    "    data = SeededRecs.objects.filter(source=content_id) \\\n",
    "               .order_by('-confidence') \\\n",
    "               .values('target', 'confidence', 'support')[:take]     \n",
    "\n",
    "    return JsonResponse(dict(data=list(data)), safe=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Content Recommenders\n",
    "\n",
    "For a movie content can include categories such as genres, actors, and directors. In other sites, it can be things such as clothing sizes, brand, style and colors, or engine sizes for cars. \n",
    "\n",
    "Content-based filtering is about extracting properties or knowledge from the content. You’ll try to extract precise definitions of each content item and represent each item as a list of values. Described like this it sounds easy, but it does pose challenges. The first image below illustrates a simple version of how to train a content-based recommender (offline), while the second figure shows how it’s used when a user arrives at your site (online).\n",
    "\n",
    "![content1](img/offline-content.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up, you need the following to make things work:\n",
    "\n",
    "\n",
    "1.  Content analyzer—Creates a model based on the content. In a way, it creates a profile for each item. It’s where the training of the model is done.\n",
    "\n",
    "2.  User profiler—Creates a user profile; sometimes the user profile is a simple list of items consumed by the user.\n",
    "\n",
    "3.  Item retriever—Retrieves relevant items found by comparing the user profiles to the item profiles as shown in figure 10.5. If the user profile is a list of items, this list is iterated, and similar items are found for each item in the user’s list.\n",
    "\n",
    "![online-content](img/online-content.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metadata about a film is everything that you can find on an IMDb page, such as genre, starring artists, and production year. It could also be something like the type of filming or the style of clothing worn by the actors in the film, or in other domains, the shade of paint on the car or the number of freckles on men on dating sites. I like to split the metadata loosely into two types:\n",
    "\n",
    "Facts\n",
    "Tags\n",
    "This isn’t a division normally used, but it’s beneficial for you to think about. Because facts are the things such as production year or starring actors in a movie that can’t be disputed, and you can also use them as input. Tags can mean different things to people and should be considered before adding them.\n",
    "\n",
    "The social internet has made it popular for people to add descriptive tags to content. Tags can be something as simple as “uplifting” or more subjective like “breaking the fourth wall.” I’ve no idea what that means, but 10 people describing Deadpool said that it was relevant, and apparently it applies to a number of films across genres and decades.\n",
    "\n",
    "Facts and tags have no clear divisions, so remember that facts are something that people often agree on, while tags can be a bit more subjective. In this light, you should probably put genres in the tag category, but that’s a matter of debate also.\n",
    "\n",
    "One of the biggest issues for developers trying to use content-based recommenders is that they can’t get the data about the items. What options do you have? You could try to build it yourself or you could hire people to go through the content and tag it. But beware, that can produce strange recommendations. Entire companies exist where people tag content for a living.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXTRACTING METADATA FROM DESCRIPTIONS\n",
    "\n",
    "In the scope of content-based filtering, news articles are interesting because they’re often only relevant for a short time (and so more affected by cold-start), which means that they’re hard to recommend using collaborative filtering. But you might still want to recommend them.\n",
    "\n",
    "Besides using popularity, you can analyze the content. One way of doing that is to look at what words are in the article, how many times each occurs, and how commonly they appear in all the news items in the database. This can be done using TF–IDF and NLP on text descriptions. An article is text, so the content is in the description, while a movie or fashion items have a description that’s written by somebody.\n",
    "\n",
    "\n",
    "  * tf(word,document) = 1 + log(word frequency)\n",
    "  \n",
    "  ![idf](img/idf.jpg)\n",
    " \n",
    "  * tf - idf(word,document) = tf(word,document)*idf(word,document)\n",
    "  \n",
    "  Besdies TF-IDF, we often use LDA to generate content features from text based on the topics that text discusses.  \n",
    "  ![LDA](img/LDA.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that LDA is sensitive to both the corpus used to train it and the choices of K, or how many topics you want.  It can contain many of the same frustrations as other unsuoervised models, such as clustering.  Make sure to use a corpus of documents talking about the same subject that you want to process.  So if you are describing fashion, don't use academic journals!\n",
    "\n",
    "![dashboard](img/LDAdashboard.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'movies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1522785b2b96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mmovie\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmovies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmovie\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmovie_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mrating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mratings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'movies' is not defined"
     ]
    }
   ],
   "source": [
    "for movie in movies:                                   \n",
    "    id = movie.movie_id\n",
    "\n",
    "    rating = ratings[id]                               \n",
    "\n",
    "    r = rating.rating\n",
    "    sum_rating += r\n",
    "    movie_dtos.append(MovieDto(id, movie.title, r))\n",
    "    for genre in movie.genres.all():                   \n",
    "\n",
    "        if genre.name in genres_ratings.keys():\n",
    "            genres_ratings[genre.name] += r - user_avg\n",
    "            genres_count[genre.name] += 1\n",
    "\n",
    "max_value = max(genres_ratings.values())               \n",
    "max_value = max(max_value, 1)                          \n",
    "max_count = max(genres_count.values())                 \n",
    "max_count = max(max_count, 1)                          \n",
    "\n",
    "genres = []\n",
    "for key, value in genres_ratings.items():              \n",
    "    genres.append((key, 'rating', value/max_value))\n",
    "    genres.append((key, 'count', genres_count[key]/max_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sample-user](img/user100.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PROS AND CONS OF CONTENT-BASED FILTERING\n",
    "Here are some things to consider when you build a content-based filtering algorithm:\n",
    "\n",
    "Pros:\n",
    "  1. New items are easy to add, overcoming the COLD START problem. Create the item feature vector, and you’re set to go.  \n",
    "  2. You don’t require much traffic. Because you can find similarity based on content descriptions, you can start recommending things from the first visit or rating.\n",
    "  3. It recommends across popularity; content-based recommenders don’t care which content is popular right now if it finds that a film nobody ever watched is as likely to be recommended as one that everybody watched.\n",
    "  \n",
    "Cons:\n",
    "  1. Conflates liking with importance. If you like science fiction films with Harrison Ford, the system will also give you films with Harrison Ford that aren’t science fiction.\n",
    "  2. No serendipity; it’s specialized.\n",
    "  3. Limited understanding of content. It might be hard to include all features that mark the aspects that make content favorable to a user, which means that the system can easily misunderstand what the user likes.\n",
    "\n",
    "\n",
    "An example of this is the first Thor movie. It could be that a user likes everything that comes out of the Shakespearian school, but normally dislikes action, but the system interprets a user liking Thor because it’s an action film. Or as Joseph Konstan says in his “Introduction to Recommender Systems”, if I like Sandra Bullock in action films and Meg Ryan in comedies, but if I hate Meg Ryan in action films and Sandra Bullock in comedies, there’s no way for that to be captured in the feature vector. That is, unless you start combining them to have a feature “Action film starring Sandra Bullock” and “Comedy starring Sandra Bullock,” and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity: \n",
    "Get into groups and discuss content and rule based recommenders.  What can you come up with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
