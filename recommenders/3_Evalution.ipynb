{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Recommender Systems\n",
    "\n",
    "## Q: Are RS a supervised or unsupervised learning problem?  \n",
    "\n",
    "This is kind of a deep question.  The answer will let us know if how we evaluate it.  \n",
    "\n",
    "![evaluation process](img/eval_rs.jpg)\n",
    "\n",
    "### Understanding my taste: Minimizing prediction error\n",
    "\n",
    "One way of measuring how well a recommender understands my tastes is by saying that it must predict whether I like an item that I know and have already rated. This can be done by measuring how often the recommender gets close to predicting the correct rating.\n",
    "\n",
    "Another way to look at it is as decision-support metrics, which divide the predictions into groups where the user will react in a similar way. If the system predicts a film above a 7 on a scale of 1–10, then I’d probably be interested in watching it. If it’s below 7 but above 3, it’s a film that I wouldn’t refuse to watch if my wife wanted to watch it. Below that I’d put my foot down and say, “No way!” If the recommender predicts a rating that’s somewhere inside the ranges that fit where I’d rate it, then it’s probably okay. If it’s in a different group, then it will make me lose time and/or lose good content. And, eventually, I’ll lose confidence in the recommender if that happens too often.\n",
    "\n",
    "### Diversity\n",
    "It’s good that popular items are recommended often, but these items might show up in too often, thus becoming even more popular, while other items don’t get shown because they aren’t yet popular. With popular items always being favored, you’re also creating what we call a filter bubble. This can cause recommendations to get stale and is a major issue in fairness of recommenders.\n",
    "\n",
    "It’s hard, however, to measure whether your system is successfully diverse and also hard for your system to be diverse when it’s personalized. Researchers have attempted to calculate a diversity measure by calculating the average dissimilarity between all pairs of the recommended items. Look at [Improving Recommendation Diversity](www.academia.edu/2655896/Improving_recommendation_diversity) by Keith Bradley and Barry Smyth for more on the subject. \n",
    "\n",
    "### Coverage\n",
    "Diversity leads nicely into coverage because the better the diversity, the better the coverage. One of the main reasons for implementing a recommender system is to enable users to navigate the full catalog, which is known as content coverage. Coverage refers both to ensuring the algorithm will recommend everything in your catalog and whether it can recommend something to all registered users, which is called user coverage.  \n",
    "\n",
    "### Serendipity\n",
    "You want to be surprised by finding things in your recommendations that you love but never knew you would. Serendipity is about giving the user that sensation, so that all their visits aren’t more of the same. It is difficult to measure, but user stuudies can help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be looking at six popular metrics: \n",
    "* Precision,\n",
    "\n",
    "![precision at K](img/patk.jpg)\n",
    "\n",
    "Precision at k is a measure of how many of the first k recommended documents are in the set of true relevant documents averaged across all users. In this metric, the order of the recommendations is not taken into account.\n",
    "\n",
    "* Average Precision, average of the precision values for the relevant items from 1 to k\n",
    "\n",
    "* Mean Average Precision (MAP), average of the Average Precision@k for all users\n",
    "\n",
    "![mean average precision at K](img/mavgp.jpg) \n",
    "\n",
    "MAP is a measure of how many of the recommended documents are in the set of true relevant documents, where the order of the recommendations is taken into account (i.e. penalty for highly relevant documents is higher).\n",
    "\n",
    "* Discounted cumulative gain (DCG) ,\n",
    "\n",
    "![DCG](img/dcg.jpg)\n",
    "* Normalized Discounted Cumulative Gain (NDCG). \n",
    "\n",
    "![NDCG](img/ndcg.jpg)\n",
    "\n",
    "NDCG at k is a measure of how many of the first k recommended documents are in the set of true relevant documents averaged across all users. In contrast to precision at k, this metric takes into account the order of the recommendations (documents are assumed to be in order of decreasing relevance).\n",
    "\n",
    "* RMSE\n",
    "\n",
    "$RMSE = \\sqrt{\\frac{\\sum_{i=0}^{N-1} (\\mathbf{y}_i - \\hat{\\mathbf{y}}_i)^2}{N}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online vs Offline tests\n",
    "\n",
    "### Offline\n",
    "Offline evaluation of recommenders\n",
    "\n",
    "* Paper testing\n",
    "* Occular test\n",
    "* Engineers\n",
    "* Testers who are specialized\n",
    "* Unspecialized testers\n",
    "\n",
    "### Online\n",
    "\n",
    "* A/B Testing\n",
    "* A/A Testing\n",
    "* Multiple flight lines and correlations\n",
    "* Unit of experiment issues (user vs clicks)\n",
    "* Bayesian Testing (Bandits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical issues\n",
    "\n",
    "## Latency\n",
    "Recommendations need to arrive in time to be used.  Make sure they are available on time, considering things like the variability of the open internet.  \n",
    "\n",
    "## Memory\n",
    "Different recommenders have significantly different memory needs.  KNN and association rules have a high memory load.  Serving quickly should be considered.  \n",
    "\n",
    "## Fallbacks\n",
    "There should always be a source of fallbacks that can be delivered quickly.  These can be simple rules based or even better non-personalized recommenders.  Often they are stored in memory at the serving layer.  \n",
    "\n",
    "## In-session updating\n",
    "Recommenders that update to match a user's in-session activity require quickly getting session level details gathered and processed.  This is often more of an engineering challenge than batch recommendations.  This data often requires a streaming pattern.  The reward is the ability to adapt to a user's tastes in session as they shop in new departments or for a gift.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
