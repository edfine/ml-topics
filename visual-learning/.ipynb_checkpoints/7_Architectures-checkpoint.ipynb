{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architectures\n",
    "\n",
    "## AlexNet\n",
    "AlexNet is a convolutional network with eight layers; the first five are convolutional layers, some of which are followed by maximum levels of pooling, and the last three are completely connected layers. It uses the ReLU activation function, which, as mentioned previously, shows better training performance compared to tanh and sigmoid. AlexNet was originally written with Compute Unified Device Architecture (CUDA) to work with GPU support.\n",
    "\n",
    "AlexNet won the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC), an international competition in the field of computer vision based on the ImageNet dataset, which contains more than a million images, accompanied by labels and designed specifically for benchmarking. After obtaining this prestigious award, he began the trend of the extensive use of CNNs in the computer vision field. 2012 was the first year in which victory went to an architecture based on CNN. In addition to the victory itself, the one that made the most noise in the computer vision community was the result achieved on the top five error, that is, the probability that, given an image, the correct label is not found among the five value predictions. AlexNet scored a 15.5% top five error, with a good 10.6 percentage point gap on the runner up, standing at 26.2%.\n",
    "\n",
    "The following diagram shows the AlexNet architecture, as published by the authors (Krizhevsky, A., Sutskever, I. and Hinton, G.E., 2012. Imagenet classification with deep CNNs. In Advances in neural information processing systems (pp. 1,097-1,105)):\n",
    "\n",
    "![AlexNet](img/alex.png)\n",
    "\n",
    "Compared to today's networks, AlexNet presents a relatively simple structure. As we've already mentioned, it consists of five convolution levels, pooling and dropout, and three fully connected levels.\n",
    "\n",
    "\n",
    "## ResNet\n",
    "The Residual Network (ResNet) represents an architecture that, through the use of new and innovative types of blocks (known as residual blocks) and the concept of residual learning, has allowed researchers to reach depths unthinkable with the classic feedforward model due to the problem of the degradation of the gradient.\n",
    "\n",
    "The following diagram shows the ResNet architecture, as published by the authors (He, K., Zhang, X., Ren, S. and Sun, J., 2016. Deep residual learning for image recognition. In proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778)):\n",
    "\n",
    "![ResNet](img/resnet.png)\n",
    "\n",
    "There are implementations with different depths, of which the deepest, in fact, counts as many as 152 levels. There is also a prototype with 1,202 levels, but it has achieved worse results due to overfitting. This architecture won ILSVRC 2015, with an error of 3.6%. To understand the value of this result, just think that the error generally achieved by a human being is around 5-10%, based on their skills and knowledge. Thanks to these results, the ResNet model is currently the state of the art in the field of computer vision.\n",
    "\n",
    "## Inception-V4, Inception-ResNet and the Impact of Residual Connections on Learning\n",
    "\n",
    "Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
